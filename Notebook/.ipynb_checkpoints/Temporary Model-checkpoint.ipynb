{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c91a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import os  \n",
    "# Importing the pytesseract module to extract text from images\n",
    "import pytesseract as tess  \n",
    "# Importing the Image module from the PIL package to work with images\n",
    "from PIL import Image  \n",
    "# Importing the convert_from_path function from the pdf2image module to convert PDF files to images\n",
    "from pdf2image import convert_from_path  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16c1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb0305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file_array,folder_path,result,my_array=np.array([0])):\n",
    "    for pdf_file in file_array:\n",
    "        address=folder_path+\"/\"+pdf_file\n",
    "        with fitz.open(address) as doc:\n",
    "            if is_text_based(doc):\n",
    "                my_array=read_generated_pdf(doc,pdf_file,result,my_array)\n",
    "            else:\n",
    "                my_array=read_scanned_pdf(address,pdf_file,result,my_array)\n",
    "    return my_array\n",
    "\n",
    "def is_text_based(doc):\n",
    "    for page in doc:\n",
    "        # Extract text from the page\n",
    "        text = page.get_text()\n",
    "        # If there's any text content, it's likely text-based\n",
    "        if text.strip():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def read_generated_pdf(doc,file_name,result,my_array):\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    output_file_name = \"C:/Users/sakhi/Desktop/NLP/result\"+\"/\"+os.path.splitext(file_name)[0] + \".txt\"\n",
    "    # Write text content to a text file\n",
    "    \"\"\"with open(output_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    return output_file_name\"\"\"\n",
    "    if result==1:\n",
    "        sentiment='successful'\n",
    "    else:\n",
    "        sentiment='unsuccessful'\n",
    "    temp=np.array([[text],[result],[sentiment]])\n",
    "    if np.array_equal(my_array,np.array([0])):\n",
    "             return temp\n",
    "    return np.concatenate((my_array,temp),axis=1)\n",
    "\n",
    "def read_scanned_pdf(file_address,file_name,result,my_array):   \n",
    "    # Store all pages of one file here:\n",
    "    pages = []\n",
    "\n",
    "    try:\n",
    "        # Convert the PDF file to a list of PIL images:\n",
    "        images = convert_from_path(file_address)  \n",
    "\n",
    "        # Extract text from each image:\n",
    "        for i, image in enumerate(images):\n",
    "          # Generating filename for each image\n",
    "            filename =  os.path.splitext(file_address)[0] + \"_\" +\"page_\" + str(i) + \".jpeg\"  \n",
    "            image.save(filename, \"JPEG\")  \n",
    "          # Saving each image as JPEG\n",
    "            text = tess.image_to_string(Image.open(filename))  # Extracting text from each image using pytesseract\n",
    "            pages.append(text)  \n",
    "          # Appending extracted text to pages list\n",
    "            os.remove(filename)\n",
    "          # Delete the generated image file\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    # Write the extracted text to a file:\n",
    "    \"\"\"output_file_name = \"C:/Users/sakhi/Desktop/NLP/result\"+\"/\"+os.path.splitext(file_name)[0] + \".txt\"  # Generating output file name\n",
    "    with open(output_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(pages))\"\"\"  \n",
    "      # Writing extracted text to output file\n",
    "\n",
    "    text=\"\\n\".join(pages)\n",
    "    if result==1:\n",
    "        sentiment='successful'\n",
    "    else:\n",
    "        sentiment='unsuccessful'\n",
    "    temp=np.array([[text],[result],[sentiment]])\n",
    "    if np.array_equal(my_array,np.array([0])):\n",
    "             return temp\n",
    "    return np.concatenate((my_array,temp),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8c0827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['judgement2022-01-03.pdf' 'judgement2022-01-13.pdf'\n",
      " 'judgement2022-01-17.pdf' 'judgement2022-01-20.pdf'\n",
      " 'judgement2022-02-08.pdf' 'judgement2022-02-10.pdf'\n",
      " 'judgement2022-04-29 (2).pdf' 'judgement2022-05-27-01.pdf'\n",
      " 'judgement2022-06-03 (2).pdf' 'judgement2022-06-03.pdf'\n",
      " 'judgement2022-06-18.pdf' 'judgement2022-06-29 (2).pdf'\n",
      " 'judgement2022-07-29.pdf' 'judgement2023-10-30.pdf'\n",
      " 'judgement2023-11-17.pdf' 'judgement2023-12-07.pdf'\n",
      " 'judgement2023-12-11.pdf' 'judgement2023-12-12.pdf'\n",
      " 'judgement2023-12-18.pdf' 'judgement2023-12-20.pdf'\n",
      " 'judgement2023-12-27.pdf' 'judgement2023-12-29 (2).pdf'\n",
      " 'judgement2024-01-02.pdf' 'judgement2024-01-04.pdf'\n",
      " 'judgement2024-01-05.pdf' 'judgement2024-01-09.pdf'\n",
      " 'judgement2024-01-10.pdf' 'judgement2024-02-21.pdf'\n",
      " 'judgement2024-02-28.pdf' 'judgement2024-03-11.pdf'\n",
      " 'judgement2024-03-23.pdf'] ['judgement2018-10-10.pdf' 'judgement2020-09-22.pdf'\n",
      " 'judgement2022-01-04.pdf' 'judgement2022-01-07.pdf'\n",
      " 'judgement2022-01-25.pdf' 'judgement2022-01-27.pdf'\n",
      " 'judgement2022-02-04.pdf' 'judgement2022-02-09.pdf'\n",
      " 'judgement2022-02-11.pdf' 'judgement2022-02-15.pdf'\n",
      " 'judgement2022-02-22.pdf' 'judgement2022-03-21.pdf'\n",
      " 'judgement2022-03-24.pdf' 'judgement2022-04-11.pdf'\n",
      " 'judgement2022-05-11.pdf' 'judgement2022-06-02-01.pdf'\n",
      " 'judgement2022-06-22.pdf' 'judgement2022-07-28.pdf'\n",
      " 'judgement2022-07-29 (2).pdf' 'judgement2022-08-04.pdf'\n",
      " 'judgement2022-08-26.pdf' 'judgement2023-11-24.pdf'\n",
      " 'judgement2023-12-04.pdf' 'judgement2023-12-13.pdf'\n",
      " 'judgement2024-01-11.pdf' 'judgement2024-01-19.pdf'\n",
      " 'judgement2024-01-29.pdf' 'judgement2024-02-09.pdf'\n",
      " 'judgement2024-02-14.pdf' 'judgement2024-03-01.pdf'\n",
      " 'judgement2024-03-04.pdf']\n"
     ]
    }
   ],
   "source": [
    "successful_path = \"C:/Users/sakhi/Desktop/NLP/Successful/\"\n",
    "unsuccessful_path=\"C:/Users/sakhi/Desktop/NLP/Unsuccessful/\"\n",
    "# Get all file names in the folder\n",
    "successful_files = os.listdir(successful_path)\n",
    "unsuccessful_files = os.listdir(unsuccessful_path)\n",
    "\n",
    "# Convert the list of file names to a NumPy array\n",
    "successful_file_array = np.array(successful_files)\n",
    "unsuccessful_file_array = np.array(unsuccessful_files)\n",
    "print(successful_file_array,unsuccessful_file_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1490c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array=get_text(successful_file_array,successful_path,1)\n",
    "my_array=get_text(unsuccessful_file_array,unsuccessful_path,0,my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec55ef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            judgement label     sentiment\n",
      "0   Consumer Disputes Redressal Forum, Kolkata - I...     1    successful\n",
      "1   1 \\n \\nIN THE TAMIL NADU STATE CONSUMER DISPUT...     1    successful\n",
      "2   Consumer Disputes Redressal Forum, Kolkata - I...     1    successful\n",
      "3   Consumer Disputes Redressal Forum, Kolkata - I...     1    successful\n",
      "4   IN THE TAMILNADU STATE CONSUMER DISPUTES REDRE...     1    successful\n",
      "..                                                ...   ...           ...\n",
      "57  STATE CONSUMER DISPUTES REDRESSAL COMMISSION H...     0  unsuccessful\n",
      "58  STATE CONSUMER DISPUTES REDRESSAL COMMISSION,\\...     0  unsuccessful\n",
      "59  1 \\n \\n \\nIN THE CIRCUIT BENCH OF THE TAMILNAD...     0  unsuccessful\n",
      "60  1 \\n \\nBEFORE THE NORTH GOA DISTRICT CONSUMER ...     0  unsuccessful\n",
      "61                                                ...     0  unsuccessful\n",
      "\n",
      "[62 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(np.transpose(my_array),columns=['judgement','label','sentiment'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20b96b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judgement</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer Disputes Redressal Forum, Kolkata - I...</td>\n",
       "      <td>1</td>\n",
       "      <td>successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 \\n \\nIN THE TAMIL NADU STATE CONSUMER DISPUT...</td>\n",
       "      <td>1</td>\n",
       "      <td>successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consumer Disputes Redressal Forum, Kolkata - I...</td>\n",
       "      <td>1</td>\n",
       "      <td>successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consumer Disputes Redressal Forum, Kolkata - I...</td>\n",
       "      <td>1</td>\n",
       "      <td>successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IN THE TAMILNADU STATE CONSUMER DISPUTES REDRE...</td>\n",
       "      <td>1</td>\n",
       "      <td>successful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           judgement label   sentiment\n",
       "0  Consumer Disputes Redressal Forum, Kolkata - I...     1  successful\n",
       "1  1 \\n \\nIN THE TAMIL NADU STATE CONSUMER DISPUT...     1  successful\n",
       "2  Consumer Disputes Redressal Forum, Kolkata - I...     1  successful\n",
       "3  Consumer Disputes Redressal Forum, Kolkata - I...     1  successful\n",
       "4  IN THE TAMILNADU STATE CONSUMER DISPUTES REDRE...     1  successful"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15cebc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Consumer Disputes Redressal Forum, Kolkata - I...\n",
       "1     1 \\n \\nIN THE TAMIL NADU STATE CONSUMER DISPUT...\n",
       "2     Consumer Disputes Redressal Forum, Kolkata - I...\n",
       "3     Consumer Disputes Redressal Forum, Kolkata - I...\n",
       "4     IN THE TAMILNADU STATE CONSUMER DISPUTES REDRE...\n",
       "                            ...                        \n",
       "57    STATE CONSUMER DISPUTES REDRESSAL COMMISSION H...\n",
       "58    STATE CONSUMER DISPUTES REDRESSAL COMMISSION,\\...\n",
       "59    1 \\n \\n \\nIN THE CIRCUIT BENCH OF THE TAMILNAD...\n",
       "60    1 \\n \\nBEFORE THE NORTH GOA DISTRICT CONSUMER ...\n",
       "61                                                  ...\n",
       "Name: judgement, Length: 62, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['judgement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab0f1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe43e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bea0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#stopwords downloaded\n",
    "#punkt downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c469f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')\n",
    "stop_words.remove('not')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87054e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(judgement):\n",
    "    # data cleaning\n",
    "    judgement = re.sub(re.compile('<.*?>'), '', judgement) #removing html tags\n",
    "    judgement =  re.sub('[^A-Za-z0-9]+', ' ', judgement) #taking only words\n",
    "  \n",
    "    # lowercase\n",
    "    judgement = judgement.lower()\n",
    "  \n",
    "    # tokenization\n",
    "    tokens = nltk.word_tokenize(judgement) # converts review to tokens\n",
    "  \n",
    "    # stop_words removal\n",
    "    judgement = [word for word in tokens if word not in stop_words] #removing stop words\n",
    "  \n",
    "    # lemmatization\n",
    "    judgement = [lemmatizer.lemmatize(word) for word in judgement]\n",
    "  \n",
    "    # join words in preprocessed review\n",
    "    judgement = ' '.join(judgement)\n",
    "    \n",
    "    return judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f36174a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judgement</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_judgement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consumer Disputes Redressal Forum, Kolkata - I...</td>\n",
       "      <td>1</td>\n",
       "      <td>consumer dispute redressal forum kolkata north...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consumer Disputes Redressal Forum, Kolkata - I...</td>\n",
       "      <td>1</td>\n",
       "      <td>consumer dispute redressal forum kolkata north...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1 cc 84 2015 filed 27th july 2015 decided 29th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRIPURA STATE CONSUMER DISPUTES REDRESSAL\\nCOM...</td>\n",
       "      <td>1</td>\n",
       "      <td>tripura state consumer dispute redressal commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEFORE THE NORTH GOA DISTRICT CONSUMER DISPUTE...</td>\n",
       "      <td>1</td>\n",
       "      <td>north goa district consumer dispute redressal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           judgement label  \\\n",
       "0  Consumer Disputes Redressal Forum, Kolkata - I...     1   \n",
       "1  Consumer Disputes Redressal Forum, Kolkata - I...     1   \n",
       "2                                                ...     1   \n",
       "3  TRIPURA STATE CONSUMER DISPUTES REDRESSAL\\nCOM...     1   \n",
       "4  BEFORE THE NORTH GOA DISTRICT CONSUMER DISPUTE...     1   \n",
       "\n",
       "                              preprocessed_judgement  \n",
       "0  consumer dispute redressal forum kolkata north...  \n",
       "1  consumer dispute redressal forum kolkata north...  \n",
       "2  1 cc 84 2015 filed 27th july 2015 decided 29th...  \n",
       "3  tripura state consumer dispute redressal commi...  \n",
       "4  north goa district consumer dispute redressal ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['preprocessed_judgement']=df['judgement'].apply(lambda judgement: data_preprocessing(judgement))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "726d033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (11, 2) (11,)\n",
      "Test data: (5, 2) (5,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df.copy()\n",
    "y = data['label'].values\n",
    "data.drop(['label'], axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3,random_state=42,shuffle=True, stratify=y)\n",
    "\n",
    "print(\"Train data:\",  X_train.shape, y_train.shape)\n",
    "print(\"Test data:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38990161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_judgement_bow shape:  (11, 2057)\n",
      "X_test_judgement_bow shape:  (5, 2057)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(min_df=1)\n",
    "\n",
    "X_train_judgement_bow = vect.fit_transform(X_train['preprocessed_judgement'])\n",
    "X_test_judgement_bow = vect.transform(X_test['preprocessed_judgement'])\n",
    "\n",
    "print('X_train_judgement_bow shape: ', X_train_judgement_bow.shape)\n",
    "print('X_test_judgement_bow shape: ', X_test_judgement_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0c6eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_judgement_tfidf shape:  (11, 2057)\n",
      "X_test_judgement_tfidf shape:  (5, 2057)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X_train_judgement_tfidf=vectorizer.fit_transform(X_train['preprocessed_judgement'])\n",
    "X_test_judgement_tfidf=vectorizer.transform(X_test['preprocessed_judgement'])\n",
    "\n",
    "print('X_train_judgement_tfidf shape: ', X_train_judgement_tfidf.shape)\n",
    "print('X_test_judgement_tfidf shape: ', X_test_judgement_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52a15651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '1' '1' '0' '0' '0' '0' '0' '1' '1']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b039d733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_judgement_bow, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_judgement_bow) #prediction from model\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7899fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' '0' '1'] ['1' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0acae156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_judgement_bow, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_judgement_bow) #prediction from model\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "208f75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' '0' '1'] ['1' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a30c3cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1,penalty='l1',solver='liblinear')\n",
    "clf.fit(X_train_judgement_bow, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_judgement_tfidf)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ac6f0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' '0' '1'] ['1' '0' '0' '0' '1']\n"
     ]
    }
   ],
   "source": [
    "print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a08bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sakhi\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clf = KMeans(n_clusters=2)\n",
    "clf.fit(X_train_judgement_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_judgement_tfidf)\n",
    "y_pred = y_pred.astype(str)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16ff978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' '0' '1'] ['0' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61014778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' '0' '1'] ['0' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c09fff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' '0' '1'] ['0' '0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a5d2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ecf86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000  # Maximum number of words to keep based on frequency\n",
    "maxlen = 100  # Maximum length of each sequence\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df['judgement'])\n",
    "X = tokenizer.texts_to_sequences(df['judgement'])\n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "y = df['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb6d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "# Step 3: Build the LSTM model\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Step 4: Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44f5d9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.3636 - loss: 0.6944 - val_accuracy: 0.8000 - val_loss: 0.6898\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.6364 - loss: 0.6895 - val_accuracy: 0.6000 - val_loss: 0.6887\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.6364 - loss: 0.6878 - val_accuracy: 0.6000 - val_loss: 0.6876\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6364 - loss: 0.6827 - val_accuracy: 0.6000 - val_loss: 0.6866\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.6364 - loss: 0.6782 - val_accuracy: 0.6000 - val_loss: 0.6856\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.6364 - loss: 0.6789 - val_accuracy: 0.6000 - val_loss: 0.6847\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7273 - loss: 0.6672 - val_accuracy: 0.6000 - val_loss: 0.6838\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6364 - loss: 0.6619 - val_accuracy: 0.6000 - val_loss: 0.6829\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7273 - loss: 0.6594 - val_accuracy: 0.6000 - val_loss: 0.6820\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7273 - loss: 0.6563 - val_accuracy: 0.6000 - val_loss: 0.6811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18507970910>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e303eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6000 - loss: 0.6811\n",
      "Test Accuracy: 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Step 7: Make predictions on new data if needed\n",
    "# predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e9a6a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14    0\n",
      "0     1\n",
      "4     1\n",
      "3     1\n",
      "7     0\n",
      "8     0\n",
      "15    0\n",
      "9     0\n",
      "12    0\n",
      "5     1\n",
      "1     1\n",
      "Name: label, dtype: int32 13    0\n",
      "10    0\n",
      "2     1\n",
      "11    0\n",
      "6     1\n",
      "Name: label, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01a696c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Obtaining dependency information for flair from https://files.pythonhosted.org/packages/af/16/536683088c7306bc51cc3cc58605759ebd83b3f7ffd05a9399f4b99c8614/flair-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading flair-0.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting boto3>=1.20.27 (from flair)\n",
      "  Obtaining dependency information for boto3>=1.20.27 from https://files.pythonhosted.org/packages/12/88/86ae378d3db210eb91a6d02c31f52690bf11510665dd9519d960024c0359/boto3-1.34.72-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.34.72-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting bpemb>=0.3.2 (from flair)\n",
      "  Obtaining dependency information for bpemb>=0.3.2 from https://files.pythonhosted.org/packages/cb/ad/54068fd577c55ea2126be5628b9ae5045fcf715bc3a41780b07f6445980d/bpemb-0.3.5-py3-none-any.whl.metadata\n",
      "  Downloading bpemb-0.3.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting conllu>=4.0 (from flair)\n",
      "  Obtaining dependency information for conllu>=4.0 from https://files.pythonhosted.org/packages/ce/3f/70a1dc5bc536755ec082b806594598a10cfffaf0de978f51d4e0e4fdfa47/conllu-4.5.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair)\n",
      "  Obtaining dependency information for deprecated>=1.2.13 from https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl.metadata\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair)\n",
      "  Obtaining dependency information for ftfy>=6.1.0 from https://files.pythonhosted.org/packages/f4/f0/21efef51304172736b823689aaf82f33dbc64f54e9b046b75f5212d5cee7/ftfy-6.2.0-py3-none-any.whl.metadata\n",
      "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting gdown>=4.4.0 (from flair)\n",
      "  Obtaining dependency information for gdown>=4.4.0 from https://files.pythonhosted.org/packages/cb/56/f4845ed78723a4eb8eb22bcfcb46e1157a462c78c0a5ed318c68c98f9a79/gdown-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: gensim>=4.2.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (4.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (0.15.1)\n",
      "Collecting janome>=0.4.2 (from flair)\n",
      "  Obtaining dependency information for janome>=0.4.2 from https://files.pythonhosted.org/packages/73/7d/70f4069f4bbf0fca023e82a1fbbade6f5216365d4fe259fee1950723eca5/Janome-0.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langdetect>=1.0.9 (from flair)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     --------- ---------------------------- 256.0/981.5 kB 5.2 MB/s eta 0:00:01\n",
      "     ----------------- -------------------- 460.8/981.5 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 696.3/981.5 kB 4.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 921.6/981.5 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 4.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=4.8.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (3.7.2)\n",
      "Collecting more-itertools>=8.13.0 (from flair)\n",
      "  Obtaining dependency information for more-itertools>=8.13.0 from https://files.pythonhosted.org/packages/50/e2/8e10e465ee3987bb7c9ab69efb91d867d93959095f4807db102d07995d94/more_itertools-10.2.0-py3-none-any.whl.metadata\n",
      "  Downloading more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting mpld3>=0.3 (from flair)\n",
      "  Obtaining dependency information for mpld3>=0.3 from https://files.pythonhosted.org/packages/95/6a/e3691bcc47485f38b09853207c928130571821d187cf174eed5418d45e82/mpld3-0.5.10-py3-none-any.whl.metadata\n",
      "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair)\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
      "  Obtaining dependency information for pytorch-revgrad>=0.2.0 from https://files.pythonhosted.org/packages/ec/e9/10b11b186b99c40213dca68cf6c38051b6704a74e1056d3f3ca4c12f14b9/pytorch_revgrad-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (2022.7.9)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (1.3.0)\n",
      "Collecting segtok>=1.5.11 (from flair)\n",
      "  Obtaining dependency information for segtok>=1.5.11 from https://files.pythonhosted.org/packages/dd/60/d384dbae5d4756e33f1750fa3472303de2c827011907a64e213e114d0556/segtok-1.5.11-py3-none-any.whl.metadata\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tabulate>=0.8.10 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (0.8.10)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (2.2.1)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (4.65.0)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
      "  Obtaining dependency information for transformer-smaller-training-vocab>=0.2.3 from https://files.pythonhosted.org/packages/1a/cf/37ccc33c7223707c92aed9d320a03fc80474ea81876b7a25096eab6fdd59/transformer_smaller_training_vocab-0.3.3-py3-none-any.whl.metadata\n",
      "  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.18.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (4.32.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from flair) (1.26.16)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair)\n",
      "  Obtaining dependency information for wikipedia-api>=0.5.7 from https://files.pythonhosted.org/packages/2f/3f/919727b460d88c899d110f98d1a0c415264b5d8ad8176f14ce7ad9db0e3b/Wikipedia_API-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair)\n",
      "  Obtaining dependency information for semver<4.0.0,>=3.0.0 from https://files.pythonhosted.org/packages/9a/77/0cc7a8a3bc7e53d07e8f47f147b92b0960e902b8254859f4aee5c4d7866b/semver-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.72 (from boto3>=1.20.27->flair)\n",
      "  Obtaining dependency information for botocore<1.35.0,>=1.34.72 from https://files.pythonhosted.org/packages/dc/7e/23b72fa8190bb16c40d7ae8ca287eb5334e3977ddd31d57ce3f49a2a108b/botocore-1.34.72-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.34.72-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from boto3>=1.20.27->flair) (0.10.0)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair)\n",
      "  Obtaining dependency information for s3transfer<0.11.0,>=0.10.0 from https://files.pythonhosted.org/packages/83/37/395cdb6ee92925fa211e55d8f07b9f93cf93f60d7d4ce5e66fd73f1ea986/s3transfer-0.10.1-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
      "Collecting sentencepiece (from bpemb>=0.3.2->flair)\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/a2/f6/587c62fd21fc988555b85351f50bbde43a51524caafd63bc69240ded14fd/sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy>=6.1.0->flair)\n",
      "  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl.metadata\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from gdown>=4.4.0->flair) (3.13.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from gensim>=4.2.0->flair) (2.0.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (2023.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (23.2)\n",
      "Requirement already satisfied: six in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from mpld3>=0.3->flair) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (2.2.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from tqdm>=4.63.0->flair) (0.4.6)\n",
      "Collecting sentencepiece (from bpemb>=0.3.2->flair)\n",
      "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/cc/07/d6951e3b4079df819d76353302fc3e76835252e7e0b6366f96a03d87ea5f/sentencepiece-0.1.99-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.3.2)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (4.25.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2.0.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (0.3.1)\n",
      "Collecting accelerate>=0.20.3 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
      "  Obtaining dependency information for accelerate>=0.20.3 from https://files.pythonhosted.org/packages/a0/11/9bfcf765e71a2c84bbf715719ba520aeacb2ad84113f14803ff1947ddf69/accelerate-0.28.0-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from jinja2->mpld3>=0.3->flair) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from accelerate>=0.20.3->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2023.3)\n",
      "Requirement already satisfied: simpful in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (2.12.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\sakhi\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim>=4.2.0->flair) (0.0.6)\n",
      "Downloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 204.8/388.3 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 388.3/388.3 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading boto3-1.34.72-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.3/139.3 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
      "Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.4/54.4 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "   ---------------------------------------- 0.0/19.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/19.7 MB 6.1 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.5/19.7 MB 5.8 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.6/19.7 MB 5.9 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.9/19.7 MB 5.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.1/19.7 MB 5.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.4/19.7 MB 5.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.6/19.7 MB 5.3 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.9/19.7 MB 5.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.1/19.7 MB 5.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.3/19.7 MB 5.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.4/19.7 MB 4.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.5/19.7 MB 4.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.8/19.7 MB 4.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.0/19.7 MB 4.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.2/19.7 MB 4.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 3.4/19.7 MB 4.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.5/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.7/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.9/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 4.1/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 4.3/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 4.5/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 4.7/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.9/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 5.1/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 5.4/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.5/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 5.8/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 6.0/19.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 6.2/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 6.4/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.6/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.8/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 7.1/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 7.3/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 7.5/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 7.7/19.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 7.9/19.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 8.2/19.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 8.4/19.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 8.6/19.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 8.8/19.7 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 9.0/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 9.2/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 9.4/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 9.6/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 9.8/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 10.0/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 10.2/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 10.4/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 10.6/19.7 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 10.8/19.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 11.0/19.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 11.3/19.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 11.5/19.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 11.8/19.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 12.0/19.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 12.2/19.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 12.5/19.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 12.7/19.7 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.9/19.7 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 13.2/19.7 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 13.4/19.7 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 13.6/19.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 13.8/19.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 14.0/19.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 14.2/19.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.5/19.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 14.7/19.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 15.0/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 15.2/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 15.5/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 15.6/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 15.9/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 16.1/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.3/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.5/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 16.6/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 16.9/19.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 17.1/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 17.3/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 17.5/19.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 17.8/19.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 18.0/19.7 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.2/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.3/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.6/19.7 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.7/19.7 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.8/19.7 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.9/19.7 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 19.0/19.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.2/19.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.5/19.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.7/19.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.7/19.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.7/19.7 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "   ---------------------------------------- 0.0/202.6 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 174.1/202.6 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 202.6/202.6 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
      "Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Downloading botocore-1.34.72-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.0 MB 5.9 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.4/12.0 MB 4.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/12.0 MB 4.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.8/12.0 MB 4.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.0 MB 4.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.2/12.0 MB 4.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/12.0 MB 4.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/12.0 MB 4.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.8/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.0/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.2/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.4/12.0 MB 4.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.0/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.2/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.2/12.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.6/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.8/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.0/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.7/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.9/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.1/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.3/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.4/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.7/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.9/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.1/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.4/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.6/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.7/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.9/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.5/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.7/12.0 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.9/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.3/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.6/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.0 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.8/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.8/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.9/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.9/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.0/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.0/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.1/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.2/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.4/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.9/12.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.0 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.0 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.2/82.2 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "   ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 163.8/977.5 kB 5.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 368.6/977.5 kB 4.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 532.5/977.5 kB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 675.8/977.5 kB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 860.2/977.5 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  972.8/977.5 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 977.5/977.5 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 0.0/290.1 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/290.1 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 174.1/290.1 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 290.1/290.1 kB 2.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: langdetect, pptree, sqlitedict\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=207bce38388145130838f659cd02f64325c4b1f5c77187aefc599855a44f4c99\n",
      "  Stored in directory: c:\\users\\sakhi\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "  Building wheel for pptree (setup.py): started\n",
      "  Building wheel for pptree (setup.py): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4613 sha256=82429bafd1b458436efa655e7f88d0b02f490fccff86f1feb9b78a6843ce08c8\n",
      "  Stored in directory: c:\\users\\sakhi\\appdata\\local\\pip\\cache\\wheels\\68\\8a\\eb\\d683aa6d09dc68ebfde2f37566ddc8807837c4415b4fd2b04c\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16901 sha256=dee3f02172c0d8bb503d057349071ab9e26d806003118b422f80b4a63459aa89\n",
      "  Stored in directory: c:\\users\\sakhi\\appdata\\local\\pip\\cache\\wheels\\73\\63\\89\\7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
      "Successfully built langdetect pptree sqlitedict\n",
      "Installing collected packages: wcwidth, sqlitedict, sentencepiece, pptree, janome, semver, segtok, more-itertools, langdetect, ftfy, deprecated, conllu, wikipedia-api, botocore, s3transfer, pytorch-revgrad, mpld3, gdown, accelerate, boto3, transformer-smaller-training-vocab, bpemb, flair\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.5\n",
      "    Uninstalling wcwidth-0.2.5:\n",
      "      Successfully uninstalled wcwidth-0.2.5\n",
      "  Attempting uninstall: more-itertools\n",
      "    Found existing installation: more-itertools 8.12.0\n",
      "    Uninstalling more-itertools-8.12.0:\n",
      "      Successfully uninstalled more-itertools-8.12.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.76\n",
      "    Uninstalling botocore-1.29.76:\n",
      "      Successfully uninstalled botocore-1.29.76\n",
      "Successfully installed accelerate-0.28.0 boto3-1.34.72 botocore-1.34.72 bpemb-0.3.5 conllu-4.5.3 deprecated-1.2.14 flair-0.13.1 ftfy-6.2.0 gdown-5.1.0 janome-0.5.0 langdetect-1.0.9 more-itertools-10.2.0 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.10.1 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.3.3 wcwidth-0.2.13 wikipedia-api-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.34.72 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b1ffb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975360551df24abba5b7efa382bcc6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sakhi\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sakhi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3897863cbf49329dcb54b3c9f67771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6694c591184bb0870f25c4791ba5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8532b01784f843e9aa2cf4735220c8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sakhi\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fe3c3a1caa4c7085b0df720f83339f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'embedding_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Define and train the LSTM model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m X_train])  \u001b[38;5;66;03m# Find maximum sentence length\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m flair_embeddings\u001b[38;5;241m.\u001b[39membedding_length  \u001b[38;5;66;03m# Get embedding dimension from Flair\u001b[39;00m\n\u001b[0;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Embedding(max_features, embedding_dim, input_length\u001b[38;5;241m=\u001b[39mmax_len))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'embedding_length'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "# Define a function to generate pre-trained Flair embeddings\n",
    "# Define a function to generate pre-trained Flair embeddings (using preprocessed text)\n",
    "def get_flair_embeddings(text):\n",
    "  sentence = Sentence(text)\n",
    "  # Load a pre-trained Transformer model (e.g., 'bert-base-uncased')\n",
    "  flair_embeddings = TransformerWordEmbeddings('bert-base-uncased')\n",
    "  flair_embeddings.embed(sentence)\n",
    "  # Extract and average word embeddings\n",
    "  embeddings = [token.embedding.detach().numpy() for token in sentence]\n",
    "  return embeddings, flair_embeddings  # Return both embeddings and flair_embeddings\n",
    "\n",
    "# Prepare training data\n",
    "X_train, flair_embeddings = zip(*df[\"preprocessed_judgement\"].apply(get_flair_embeddings))\n",
    "y_train = df[\"label\"].tolist()\n",
    "\n",
    "# Define and train the LSTM model\n",
    "max_len = max([len(sent) for sent in X_train])  # Find maximum sentence length\n",
    "embedding_dim = flair_embeddings.embedding_length  # Get embedding dimension from Flair\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(lstm_units))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))  # Output layer for binary classification\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=epochs)\n",
    "\n",
    "# Make predictions on new data (optional)\n",
    "\"\"\"new_text = \"Your new text here\"\n",
    "new_embedding = get_flair_embeddings(new_text)\n",
    "prediction = model.predict(np.array([new_embedding]))[0][0]\n",
    "print(f\"Predicted label: {prediction:.2f}\")  # Probability of being successful\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26cb8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c53d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
